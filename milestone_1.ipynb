{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJmeIAXzRPtW73f9A1t6Hy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darkeagle011110/A.I.-Resume-Matchmaker/blob/main/milestone_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KN2GUr6rYzuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359f2168-7981-4b91-95f0-7a7846e5f85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.5\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf\n",
        "!pip install pandas\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(file_path)\n",
        "\n",
        "    # Initialize an empty string to hold the text\n",
        "    text = \"\"\n",
        "\n",
        "    # Iterate through each page and extract text\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_and_normalize_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and normalizes text data by converting to lowercase, removing special characters,\n",
        "    and removing stopwords.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be cleaned and normalized.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned and normalized text.\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def consolidate_and_save_data(cleaned_resume_text, cleaned_jd_text, file_path):\n",
        "    \"\"\"\n",
        "    Consolidates cleaned resume and job description text into a pandas DataFrame and saves it to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        cleaned_resume_text (str): The cleaned text from the resume.\n",
        "        cleaned_jd_text (str): The cleaned text from the job description.\n",
        "        file_path (str): The path to save the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'Type': ['Resume', 'Job Description'],\n",
        "        'Cleaned Text': [cleaned_resume_text, cleaned_jd_text]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    df.to_csv(file_path, index=False)\n",
        "\n",
        "# Example usage:\n",
        "resume_path = '/final resume.pdf'\n",
        "job_description_path = '/data anyls respon.pdf'\n",
        "csv_output_path = '/output.csv'\n",
        "\n",
        "# Step 1: Extract text from PDF files\n",
        "resume_text = extract_text_from_pdf(resume_path)\n",
        "job_description_text = extract_text_from_pdf(job_description_path)\n",
        "\n",
        "# Step 2: Clean and normalize the text data\n",
        "cleaned_resume_text = clean_and_normalize_text(resume_text)\n",
        "cleaned_jd_text = clean_and_normalize_text(job_description_text)\n",
        "\n",
        "# Step 3: Consolidate and save the cleaned data to a CSV file\n",
        "consolidate_and_save_data(cleaned_resume_text, cleaned_jd_text, csv_output_path)\n",
        "\n",
        "print(\"Data has been successfully cleaned, normalized, and saved to:\", csv_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXszVMKEmzFK",
        "outputId": "b61131ff-f964-4274-fc5a-92527a1fc842"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully cleaned, normalized, and saved to: /output.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}